import json
import os
import time
import subprocess
import hashlib

# --- CONFIGURATION ---
INPUT_FILE = "raw_links.txt"   
CACHE_FILE = "dead_cache.txt"   
CLEANUP_LOG = "last_cleanup.txt"

# Пороги сортировки (Настраиваемые параметры)
MAX_PING = 300       # Максимальный пинг для Stable Reserve (мс)
ELITE_PING = 150     # Максимальный пинг для Elite (мс)
SPEED_MIN = 3.0      # Минимальная скорость для Speed Master (Мбит/с)

def get_md5(text):
    """Generates a stable unique ID for a proxy link."""
    return hashlib.md5(text.encode()).hexdigest()

# --- 1. CACHE LIFECYCLE MANAGEMENT ---
def manage_cache_lifecycle():
    """Removes cache and old result files every 72 hours to refresh the pool."""
    now = time.time()
    three_days = 259200 
    
    if os.path.exists(CLEANUP_LOG):
        with open(CLEANUP_LOG, "r") as f:
            try:
                last_clean = float(f.read().strip())
            except:
                last_clean = 0
    else:
        last_clean = 0
        
    if now - last_clean > three_days:
        print("[System] 72h Cleanup triggered. Refreshing data pool...")
        if os.path.exists(CACHE_FILE): os.remove(CACHE_FILE)
        # Clean old results to ensure fresh data
        for f in ["Elite_Gemini.txt", "Speed_Master.txt", "Stable_Reserve.txt"]:
            if os.path.exists(f): 
                with open(f, "w") as empty: pass
            
        with open(CLEANUP_LOG, "w") as f:
            f.write(str(now))

# --- 2. DATA INPUT & FILTERING ---
def load_and_filter_input():
    """Loads links, removes duplicates and filters against the dead nodes cache."""
    if not os.path.exists(INPUT_FILE):
        print(f"[Error] {INPUT_FILE} not found! Please create it.")
        return []
    
    # Load and deduplicate
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        links = list(set(line.strip() for line in f if line.strip()))
    
    # Load dead pool IDs
    dead_pool = set()
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r") as f:
            dead_pool = set(line.strip() for line in f if line.strip())
            
    # Filter links that are not in dead pool
    filtered = [l for l in links if get_md5(l) not in dead_pool]
    print(f"[System] Total links: {len(links)}, New/Alive to check: {len(filtered)}")
    return filtered

# --- 3. CORE INTEGRATION (LITESPEEDTEST) ---
def execute_lite_benchmark():
    """Runs the modern core v0.17.1 and parses the JSON results."""
    links = load_and_filter_input()
    if not links:
        return []

    temp_input = "temp_batch.txt"
    with open(temp_input, "w") as f:
        f.write("\n".join(links))

    print(f"[System] Starting benchmark. Target: Gemini AI Access...")
    
    # Run core with Gemini check and JSON output
    # -f: input file, -p: custom check URL, -o: output format
    try:
        subprocess.run([
            "./lite-speedtest",
            "-f", temp_input,
            "-p", "https://gemini.google.com",
            "-o", "json"
        ], capture_output=True, timeout=600) # 10 min timeout
    except Exception as e:
        print(f"[Error] Core execution failed: {e}")
        return []

    results_file = "result.json"
    if not os.path.exists(results_file):
        print("[Error] Core failed to produce result.json")
        return []

    with open(results_file, "r", encoding="utf-8") as f:
        try:
            return json.load(f)
        except Exception as e:
            print(f"[Error] Failed to parse results: {e}")
            return []

# --- 4. CLASSIFICATION & PERSISTENCE ---
def triple_tier_classifier():
    """Classifies nodes into 3 tiers and updates the dead nodes cache."""
    nodes = execute_lite_benchmark()
    
    elite_gemini = []
    speed_master = []
    stable_reserve = []
    dead_ids = []

    for node in nodes:
        link = node.get("link")
        ping = node.get("ping", 999)
        speed = node.get("speed", 0)
        # This field comes from the -p parameter in lite-speedtest
        gemini_ok = node.get("google_check", False)

        node_id = get_md5(link)

        # 1. Check for dead or too slow nodes
        if ping >= MAX_PING or ping == 0:
            dead_ids.append(node_id)
            continue

        # 2. Tier 1: Elite (Fast + Gemini access)
        if ping < ELITE_PING and gemini_ok:
            elite_gemini.append(link)
        # 3. Tier 2: Speed Master (Fast, but maybe no Gemini)
        elif ping < 200 and speed > SPEED_MIN:
            speed_master.append(link)
        # 4. Tier 3: Stable Reserve (Slow but alive)
        elif ping < MAX_PING:
            stable_reserve.append(link)

    # Save results (Overwriting with current best)
    with open("Elite_Gemini.txt", "w") as f:
        f.write("\n".join(elite_gemini))
    with open("Speed_Master.txt", "w") as f:
        f.write("\n".join(speed_master))
    with open("Stable_Reserve.txt", "w") as f:
        f.write("\n".join(stable_reserve))

    # Append new dead IDs to cache
    if dead_ids:
        with open(CACHE_FILE, "a") as f:
            f.write("\n".join(dead_ids) + "\n")
    
    print(f"[Summary] Elite: {len(elite_gemini)} | Speed: {len(speed_master)} | Reserve: {len(stable_reserve)}")

if __name__ == "__main__":
    manage_cache_lifecycle()
    triple_tier_classifier()
    print("[Success] Audit cycle complete.")
